\documentclass[10pt]{article}

\usepackage[margin=0.25in]{geometry}

% Enable (uncolored) cross-reference hyperlinks
\usepackage[colorlinks=false]{hyperref}

\usepackage{multicol}

\usepackage{titlesec}  % Used to adjust title heading
% Format:  \titlespacing{command}{left spacing}{before spacing}{after spacing}[right]
\titlespacing\subsection{0pt}{6pt plus 4pt minus 0pt}{4pt plus 2pt minus 0pt}

% Imported via UltiSnips
\usepackage{color}
\newcommand{\colortext}[2]{{\color{#1} #2}}
\newcommand{\red}[1]{\colortext{red}{#1}}
\newcommand{\blue}[1]{\colortext{blue}{#1}}
\newcommand{\green}[1]{\colortext{green}{#1}}

% Imported via UltiSnips
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\usepackage{amsfonts}  % Used for \mathbb and \mathcal
\usepackage{amssymb}
\usepackage[bb=boondox]{mathalfa}

% Imported via UltiSnips
\usepackage{mathtools} % for "\DeclarePairedDelimiter" macro
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

% Imported via UltiSnips
\usepackage[noend]{algpseudocode}
\usepackage[Algorithm,ruled]{algorithm}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\setlength{\parindent}{0cm}
\setlength{\itemsep}{0cm}

\begin{document}

\begin{center}
  {\Large CIS572 Midterm Study Guide} \\\vspace{6pt}
  Zayd Hammoudeh
\end{center}


\begin{multicols}{3}
\textbf{Goal of Learner}: Minimize \red{expected loss}

\begin{itemize}
  \item \textbf{Test Data}: Quantifies generalizability of an algorithm
\end{itemize}

  \section*{Definitions}

  \begin{itemize}
    \item \textbf{Hyperparameter}: Any parameter whose value is set before the learning process begins.
    \item \textbf{Hypothesis Space}: Specifies set of all functions a learning algorithm would search when constructing a model.
    \item \textbf{Margin}: \red{Two} times the distance from the hyperplane to the nearest training example
    \item \textbf{Overfitting}: Hypothesis $h$ overfits the data if there exists an alternate hypothesis $h'$ such that $error_{train}(h) < error_{train}(h')$ but $error_{\mathcal{D}}(h) > error_{\mathcal{D}}(h')$ for the entire data distribution $\mathcal{D}$.
     \begin{itemize}
       \item Fitting training data very closely but not generalizing well
     \end{itemize}
  \end{itemize}

  \section{Introduction}

  \begin{itemize}
    \item \textit{Combinatorial Optimization}: Find optimal object within a finite set. (e.g.,~greedy search)
    \item \textit{Convex Optimization}: Gradient descent.
    \item \textit{Constrained Optimization}: Linear programming
  \end{itemize}

  \textbf{Reinforcement Learning}: Interact with the world through ``actions'' and find a \textit{policy} of behavior that optimizes some ``rewards''

  \begin{itemize}
    \item \textit{Classification}: Predict a discrete/categorical value from a predefined set of values
    \item \textit{Regression}: Predict a continuous/real value
    \item \textit{Structured Prediction}: Predict a complex output as a sequence or tree
  \end{itemize}

  \noindent
  \textbf{Performance Measure Selection}: Depends on the problem to be solved.

  \section{Decision Trees}

  \textbf{Inner Node}: Test and branch according to value of particular feature ${x_j \in \mathbf{x}}$

  \noindent
  \textbf{Leaf Nodes}: Specify class of $\mathbf{x}$

  \begin{algorithm}[h]
    \caption{ID3}\label{alg:ID3}
    \begin{algorithmic}[1]
      \If{All examples in same class $c$}
        \Return Leaf node with label $c$
      \EndIf
      \State Select feature $x_j$
      \State Generate new node $dt$ with test on $x_j$
      \State Split training set based on value of $x_j$
      \State Call algorithm recursively
    \end{algorithmic}
  \end{algorithm}

  \noindent
  \textbf{Entropy}: Quantifies uncertainty.  Can be seen as negative information.

  \[ Gain(S,x) = H(S) - H(S\vert x) \]

  \[ H(S\vert x) = \sum_{v\in Values(x)} \frac{\abs{S_v}}{\abs{S}} H(S_v) \]

  \noindent
  \textbf{Gain Ratio}: Measures how broadly \& uniformly attribute splits the data.  Relates information gain to how data is split.
  \begin{itemize}
    \item Generally more reliable than information gain.
  \end{itemize}

  \[SplitInfo(S,A) = -\sum_{i=1}^{c} \frac{\abs{S_i}}{\abs{S}} log_{2} \frac{S_i}{S} \]

  \[ GainRatio(S,A) = \frac{InfoGain{S,A}}{SplitInfo(S,A)} \]

  \textbf{Hypothesis Space}: All possible finite discrete-valued functions.  Definitely included given a finite training set.

  \textbf{Disadvantages of ID3 and C4.5}
  \begin{itemize}
    \item Uses all training examples so sensitive to error/noises in training data.
    \item Greedy variable selection that may converge to only a local optimum.
  \end{itemize}

  \textbf{Parity Features}: Decision trees cannot distinguish random features from parity (e.g.,~XOR) features

  \textbf{Occam's Razor}: Prefer the simplest hypothesis that fits the data

  \textbf{When to Use}:
  \begin{itemize}
    \item Instances describable by attribute value pairs.
    \item Categorical output variable.
    \item Interpretable learner is required.
    \item Training data may contain errors
  \end{itemize}

  \section{Inductive Learning}

  \textbf{Performance Measure}: Quantifies how different/bad a system's prediction is versus the truth.
  \begin{itemize}
    \item Achieved via the loss function.
  \end{itemize}

  Training and test data can be seen as being sampled from a probability distribution.
  \[(\mathbf{x},y)\sim P(x,y)\]
  \begin{itemize}
    \item $P$ is unknown
    \item Find a function $f:\mathbf{x} \rightarrow y$
  \end{itemize}

  Average loss on training data may be misleading since learning may \textit{memorize the data} and \textit{overestimate performance}

  \textbf{Data}: Labeled instances consisting of training set, hold out set, \& test set.

  \textbf{Features}: Attribute-value pairs which characterize each $\mathbf{x}$

  \textbf{Hold-out}/\textbf{Validation Set}: Used for hyperparameter tuning.
  \begin{itemize}
    \item Never ``peek'' at the test set
  \end{itemize}

  \textbf{Generalization Goal}: Classifier that performs well on \textit{test data}, i.e.,~minimizes the \textit{expected loss}, not the training loss.

  \textbf{Reasons a Dataset is not Learnable}
  \begin{itemize}
    \item Noise in training data
    \item Same input may correlate to multiple output labels
    \item Provided features insufficient for learning
  \end{itemize}

  \section{$k$-Nearest Neighbors}

  \textbf{Parametric Learner}: A particular functional form is assumed.
  \begin{itemize}
    \item May have high bias if real data has different functional form.
  \end{itemize}

  \textbf{Non-parametric Learner}: Distribution or density estimate is data driven with relatively few assumptions.
  \begin{itemize}
    \item \textit{Model complexity is data driven.}
  \end{itemize}

  \textbf{Learning Algorithm}: Store training examples

  \textbf{Prediction Algorithm}: Classify new example by finding the example(s) nearest to it.
  \begin{itemize}
    \item \textit{Option~\#1}: Use the most frequently occurring class.
    \item \textit{Option~\#2}: Assign each of $k$ neighbors weight based on distance.
  \end{itemize}

  \textbf{Decision Boundary}: Not explicit in KNN\@. Decision boundary form a subset of the Voronoi diagram

  \textbf{Curse of Dimensionality}: ``Neighborhood'' becomes very large in high dimensional spaces
  \begin{itemize}
    \item Distance becomes less significant as dimension increases
  \end{itemize}

  \textbf{Effect of $k$}
  \begin{itemize}
    \item \textit{Increasing $k$}: Reduces variance and increases bias. Can lead to underfitting too high.
    \item \textit{Decreasing $k$}: Can lead to overfitting if too low.
  \end{itemize}

  \textbf{Standardization}: Make all features equally important by dividing values by their standard deviation.
  \begin{itemize}
    \item No need to subtract mean since canceled out in Euclidean distance.
  \end{itemize}

  \subsection*{Distance Metrics}
  \textbf{Minkowski}: $L_{\lambda} = \left( \sum_{k=1}^{p} \abs{x_k(i) - x_k(j)}^{\lambda}\right)^{\frac{1}{\lambda}}$

  \vspace{8pt}\textbf{Manhattan}: $L_1 = \sum_{k=1}^{p} \abs{x_k(i) - x_k(j)}$

  \vspace{8pt}$L_{\infty} = \max_{k}\abs{x_k(i),x_k(j)}$

  \vspace{8pt}\textbf{Weighted Euclidean}:
  \vspace{-4pt}
  \[ d(i,j) = \left( \sum_{k=1}^{p} w_k \left(x_k(i) - x_k(j)\right)^{2} \right)^{1/2} \]

  \subsection*{When to Use}
  \begin{itemize}
    \item Lots of training data
    \item Instances map to points in $\mathbb{R}^n$
    \item Less than 20~features
  \end{itemize}

  \subsection*{Advantages}
  \begin{itemize}
    \item Fast to train
    \item Easy to implement
    \item Learn complex and flexible decision boundaries, i.e.,~target functions
    \item No loss of information
  \end{itemize}

  \subsection*{Disadvantages}
  \begin{itemize}
    \item Slow query time $O(nd)$
    \item High memory requirements
    \item Irrelevant or uncorrelated features must be removed
    \item Distance function must be carefully chosen
    \item Easily tricked by irrelevant attributes
    \item Typically cannot handle more than 30~features
  \end{itemize}

  \section{Perceptron}

  Creates a linear decision boundary.

  \subsection*{Update Rule}
  \textbf{Initial}:

  $\mathbf{w}^{(t)} = \vec{0}$ and $b^{(t)}=0$

  \textbf{Update}:
  \[ \mathbf{w}^{(t+1)} = \begin{cases}
                            \mathbf{w}^{(t)}                  & y = \sign(\mathbf{w}^{(t)}\cdot\mathbf{x} +b) \\
                            \mathbf{w}^{(t)} + y\mathbf{x}    & \text{Otherwise}
                          \end{cases} \]

  \[ b^{(t+1)} = \begin{cases}
                   b^{(t)}      & y = \sign(\mathbf{w}^{(t)}\cdot\mathbf{x} +b) \\
                   b^{(t)} + y  & \text{Otherwise}
                 \end{cases} \]

  \subsection*{Disadvantages}

  \begin{itemize}
    \item Inherently weights later examples more than earlier ones.  \textit{Solutions}:
      \begin{itemize}
        \item Shuffle training examples for each epoch
        \item Use a weighted perceptron
      \end{itemize}
    \item \textit{Mediocre Generalization}: Finds a ``barely'' separating solution
    \item \textit{Overtraining}: Test/validation accuracy rises then falls
  \end{itemize}
  \section{Linear Regression}

  \textbf{Cost Function}: Measures the difference between the predicted and true values.

  \textbf{Minimizing the Cost for Linear Regression}: Take the gradient and set the gradient equal to $\vec{0}$

  \begin{align*}
    J &= \frac{1}{2}(Xw-y)^{T}(Xw-y) \\
    J &= \frac{1}{2}(w^{T}X^{T}Xw - w^{T}X^{T}y -y^{T}Xw + y^{T}y) \\
    \nabla_{w}J &= X^{T}Xw-X^{T}y
  \end{align*}

  Setting equal to $\vec{0}$ yields:

  If errors below to a normal distribution, value \red{minimizing least square error equivalent to maximum likelihood}

  \textbf{\blue{Maximum a posteriori}} (MAP): Most probable hypothesis given the data
  \begin{enumerate}
    \item Derived \red{from maximum likelihood}
    \item $P(D)$ unrelated to $h$ so ignored in MAP
  \end{enumerate}

  \subsection*{Closed Form}

  \begin{align*}
    X^{T}Xw - X^{T}y &= 0 \\
    X^{T}Xw &= X^{T}y \\
    w &= (X^{T}X)^{-1}X^{T}y \\
  \end{align*}

  \subsection*{Advantages}

  \begin{itemize}
    \item Closed form optimal solution
    \item Unique global optimum
  \end{itemize}

  \subsection*{Potential Problems}

  \begin{itemize}
    \item $X^{T}X$ must be invertible
    \item May need to transform data to make it linearly separable
  \end{itemize}

  \section{Logistic Regression}

  \red{Probabilistic Model}: Generates a condition probability $\Pr[Y\vert X]$

  \textbf{Logistic Function}: $\sigma: \mathbb{R} \rightarrow (0,1) $
  \[ \sigma(z) = \frac{1}{1+\exp(-z)} \]

  \textbf{Decision Function}:

  \[ \Pr[Y\vert X] = \frac{1}{1+\exp{-(w^{T}x + b)}} \]

  \textbf{Likelihood}: $L(h) = P(D\vert h)$

  Since samples are i.i.d.:
  \begin{align*}
    P(D\vert h) &= \prod_{i=1}^m P(\langle x_i,y\rangle \vert h) \\
                &= \prod_{i=1}^{m}P(y_i\vert x_i;h) P(x_i)
  \end{align*}

  $P(x_i)$ is a constant and since $log$ is concave we can apply it to the probability:

  \section{Support Vector Machine}

  \textbf{Linear SVM}: Creates a linear decision boundary.

  \textbf{Margin}: \red{Two} times the distance from the hyperplane to the nearest training example

  \textbf{Normal}: Vector $w$ is orthogonal to the decision boundary.  Use its unit vector $\frac{w}{\norm{w}}$

  \subsection*{Support Vectors}

  \textit{Support vectors}: Define the decision boundary. \red{Lie on the edge of the margin}

  Distance to the hyperplane is:

  \[ \gamma_i = y_i \left(\frac{w}{\norm{w}} x_i + \frac{w_0}{\norm{w}}\right) \]

  Define $M=\min_i \gamma_i$ where $2M$ is the \blue{margin of the hyperplane}

  \textbf{Goal}: $\max_{w,w_0}\min_{i} \gamma_i$

  \subsection*{Primal Formulation}
  \begin{enumerate}
    \item Define $\norm{w}M = 1$.  Reformulate as $\min$ by instead maximizing $\norm{w}$
    \item Maximize $\norm{w}^2$ which is same as maximizing $\norm{w}$
  \end{enumerate}

  \blue{Primal Formulation}
  \begin{align*}
    \text{min }    &\norm{w}^{2}\\
    \text{w.r.t. } &w,w_0 \\
    \text{s.t. }   &y_i(w\cdot \phi(x_i) +w_0) \geq 1
  \end{align*}

  \red{Quadratic programming} problem \& convex

  \subsection*{Dual Formulation}
  \begin{align*}
    \text{max }    &\sum_{i}\alpha_i - \frac{1}{2}\sum_{i,j}y_{i}y_{j}\alpha_{i}\alpha_{j}(x_i\cdot x_j) \\
    \text{w.r.t. } &\alpha \\
    \text{s.t. }   &\alpha_i \geq 0 \\
                   & \sum_i \alpha_i y_i = 0
  \end{align*}

  \blue{Support Vector}: Any $x_i$ with $\alpha_i>0$

  \textbf{Output Prediction}:

  \[h(x) = \sign\bigg(\sum_{i=1}^{m}\alpha_{i}y_{i}(x_i\cdot x) + w_0\bigg) \]

  \subsection*{Kernels}
  \textbf{Kernel}: Function $K:\mathbb{R}^{n}\times\mathbb{R}^{n} \rightarrow \mathbb{R}$ that corresponds to a \blue{feature mapping} $\phi$:

  \[K(x_1,x_2)=\phi(x_i)\cdot\phi(x_j) \]

  Dot products are defined as:

  \[\phi(x_i) \cdot \phi(x_j) = \cos\angle(x_i,x_j) \]

  where $\angle$ is the angle between vectors $x_i$ and $x_j$ making a kernel a form of \red{similarity}.

  \textbf{\blue{Dual Formulation}}: Replace $x_i$ \& $x$ with $\phi(x_i)$ \& $\phi(x)$ respectively

  \textbf{\blue{Kernel Trick}}: Solve the dual with the kernel function eliminates needs to find feature mapping directly.

  \subsubsection*{Kernel Types}

  \textbf{Linear}: $K(x_i,x_j)=x_{i}^{T}x_j$

  \textbf{Polynomial}: $K(x_i,x_j)=(1+ x_{i}^{T}x_j)^p$

  \textbf{Gaussian} (RBF): \\
  $K(x_i,x_j)=\exp\left(-\frac{\norm{x_{i} - x_{j}}^2}{2\sigma^2}\right)$

  \textbf{Sigmoid}: $K(x_i,x_j) = \tanh(\beta_{0} x_{i}^{T}x_{j} +\beta_1)$

  \textbf{Custom Kernel Function}: May be application specific

  \subsection{Soft-margin SVM}

  \textbf{Use Case}: Data that is \red{noisy} or \red{not linearly separable}

  \textbf{\blue{Soft Error}}: $\sum_{i}\xi_i$

  Allow for misclassified samples.  One soft error term $\xi_i$ for each training sample~$i$

  \begin{align*}
    \text{min }    &\frac{1}{2}\norm{w}^{2} + C\sum_i\xi_i \\
    \text{w.r.t. } &w,w_0, \xi_i \\
    \text{s.t. }   &y_i(w\cdot \phi(x_i) +w_0) \geq 1-\xi_i \\
                   &\xi_i \geq 0
  \end{align*}

  If $C=0$, no penalty for soft errors.  If $C$ large, emphasis is on soft errors.

  \subsection*{Multi-class SVM}

  \textbf{One-vs-All}
  \begin{itemize}
    \item $n$ total classifiers
    \item Train each classifier for one class versus all other classes
    \item Choose the class with the largest margin
  \end{itemize}

  \textbf{One-vs-One}
  \begin{itemize}
    \item $\binom{n}{2}$ total classifiers
    \item Train classifier between each pair of classes
    \item Choose the class selected by most classifiers
  \end{itemize}

  \subsection*{Overfitting Symptoms}

  \begin{itemize}
    \item Large number of instances are support vectors
    \item Low margin
  \end{itemize}

  \section{Linear Models}

  \subsection{Regularizers}

  \textbf{Role of Regularization}: \red{Adjust the inductive bias}

  \vspace{6pt}\textbf{Challenge of Regularization}: Choosing the right $\lambda$ is hard and requires cross validation.

  \textbf{$p$-Norms}:
  \begin{itemize}
    \item For $p<1$: More likely to get sparse representations
  \end{itemize}

  \textbf{L1}: Lasso
  \begin{itemize}
    \item Convex but not smooth
    \item More resistant to outliers
    \item Often used for feature selection as promotes sparse feature representations.  Helps make irrelevant features have zero weight.
    \item Generally takes longer to train and less accurate
  \end{itemize}

  \textbf{L2}: Ridge
  \begin{itemize}
    \item Convex and smooth
    \item Generally more accurate
  \end{itemize}

  \begin{itemize}
    \item $L_1$ more resistant to outliers than $L_2$ since the penalty of outliers increases quadratically.
    \item $L_1$ used as a feature selection method.
    \item $L_2$ tends to lead to really small features all close to zero.
  \end{itemize}

  \subsection{Loss Function}

  \textbf{Convex Loss Functions}: Guaranteed to converge near global minimum with gradient descent

  \textbf{Smooth}: Property of the number of derivatives that are continuous.
  \begin{itemize}
    \item Small change in predicted value should lead to small change in loss.
  \end{itemize}

  \textbf{0/1 Loss}: (Very non-smooth)\\
  $ \ell^{(\text{0/1})}(y,\hat{y}) = \mathbb{1}[y\hat{y} \leq 0] $

  \textbf{Hinge Loss}:
  $ \ell^{(\text{hin})}(y,\hat{y}) = \max\{0, 1-y\hat{y}\} $

  \textbf{Logistic}:
  $ \ell^{(\text{log})}(y,\hat{y}) = -\log(1+\exp[-y\hat{y}]) $

  \textbf{Exponential}:
  $ \ell^{(\text{exp})}(y,\hat{y}) = \exp[-y\hat{y}] $

  \textbf{Square Loss}:
  $ \ell^{(\text{sqr})}(y,\hat{y}) = (y - \hat{y})^{2} $

  \section{Neural Networks}

  \textbf{Threshold Logic Unit}:
  \vspace{-4pt}
  \[ f(\mathbf{x}) = \begin{cases}
                          1 & \mathbf{w}^{T}\mathbf{x} + b \geq 0 \\
                          0 & \text{Otherwise}
                      \end{cases} \]

  \textbf{OR}: $or(a,b) =\mathbb{1}(a + b -0.5\geq0)$

  \vspace{6pt}\textbf{AND}: $and(a,b) =\mathbb{1}(a + b -1.5\geq0)$

  \vspace{6pt}\textbf{NOT}: $not(a) =\mathbb{1}(-a + 0.5\geq0)$

  \textbf{Computational Graph Nodes}:
  \begin{itemize}
    \item Dot (vector)
    \item Add (scalar)
    \item $sign$ (scalar)
  \end{itemize}

  \textbf{Leave-One-Out-Cross Validation}: Used when training time is manageable and training dataset is small.
  \begin{enumerate}
    \item For each $i$ in dataset of size $m$
      \begin{enumerate}
        \item Leave out item $i$ from training set and use as validation
        \item Use all other data to train weight vector
        \item Measure error predicting label of $i$\textsuperscript{th} sample
      \end{enumerate}
    \item Repeat $m$ times
  \end{enumerate}

  \subsection*{Gradient Descent}

  Generally \red{steepest descent} unless using higher order methods.

  \textbf{Batch Gradient Descent}:
  \begin{itemize}
    \item Complexity of update grows linearly with size of batch (dataset).
    \item Optimizes the \blue{empirical error}
  \end{itemize}

  \textbf{Stochastic Gradient}: Gradient estimate is unbiased. Minimizes the \red{expected risk} not the empirical risk.  May jump out of local minima. High variance on gradient

  \textbf{Minibatch Gradient Descent}: Lower variance on gradient.
  \begin{itemize}
    \item Approximates gradient whole dataset.
    \item May jump out of \textit{shallow} local minima.
  \end{itemize}

  \subsection*{Multilayer Perceptron}

  \begin{itemize}
    \item If no non-linearities, layers in series behave as if a single layer.
  \end{itemize}

  \subsection*{Loss Functions}

  \begin{itemize}
    \item \textbf{Cross Entropy}:
  \end{itemize}
  \vspace{-5pt}
  \[ H(p,q) = \mathbb{E}_{p}[-\log(q)] \]
  \begin{itemize}
    \item \textbf{Logistic Loss}:
  \end{itemize}
  \vspace{-5pt}
  \[ \mathcal{L}(w,b) = -\sum_{(\mathbf{x}_i,y_i)\in\mathcal{D}} \log\bigg( \sigma\Big(y_i(\mathbf{w}^T\mathbf{x}+b)\Big)\bigg) \]

  \subsection*{Activation Functions}

  \begin{itemize}
    \item $\sign(x)$
    \item $\sigma(x) = \frac{1}{1+\exp{-x}}$
    \item $ReLU(x) = \max\{0,x\}$
    \item $SoftPlus(x) = SmoothReLU(x) = \log(1+e^{x})$. Gradient is the sigmoid function. Differentiable version of ReLU\@.
    \item $ELU(x)= \begin{cases}
                      x & x \geq 0 \\
                      \alpha(e^{x} - 1) & \text{Otherwise}
                    \end{cases}$
  \end{itemize}

  \section*{Inductive Biases}

  \textbf{Inductive Bias}: Fundamental set of assumptions made by the learner about the target function that enables it to generalize beyond the training data.
  \begin{itemize}
    \item Many different functions $h\in\mathcal{H}$ may have same training error.
  \end{itemize}

  \textbf{Inductive Bias of Each Learner}
  \begin{itemize}
    \item \textit{Decision trees}: Imposed on search process. Shorter trees preferred over longer ones. Prefer to place variables with higher information gain closer to root.
    \item \textit{Human}: Past experiences and limit of person's comprehension
    \item \textit{k-Nearest Neighbors}: Classification of example will be most similar to classification of other instances nearby (in terms of distance metric).
    \item \textit{Linear Regression}: Relationship between $\mathbf{X}$ and $y$ is linear.
    \item \textit{Perceptron}: Linearly separable.  Each input variable votes independently towards final classification.
    \item \textit{Support Vector Machine}: Distinct classes tend to be separated by wide margins.
  \end{itemize}

  \section*{Stopping Overfitting}
  \begin{itemize}
    \item \textit{Decision Tree}: Pre-prune (early stopping) or post-pruning.  Maximum number of remaining examples before creating a leaf.
    \item \textit{$k$~Nearest Neighbors}: Increase~$k$
    \item \textit{Logistic Regression}: $L_2$ regularization
    \item \textit{SVM}: Regularization parameter $c$ for the slack variables
  \end{itemize}
\end{multicols}
\end{document}
