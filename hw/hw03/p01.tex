\newcommand{\featureOp}{normalized}

\begin{problem}
An analyst wants to classify a number of customers based on some given attributes: total number of accounts, credit utilization (amount of used credit divided by total credit amount), percentage of on-time payments, age of credit history, and inquiries (number of time that the customer requested a new credit account, whether accepted or not). The analyst acquired some labeled information as shown in the following table:

\begin{table}[h]
  \centering
  \caption{Raw training data for problem~\#1}\label{tab:P01:RawTrain}
  \begin{tabular}{|c||c|c|c|c|c|c|}
    \hline
    ID & Total    & Utilization & Payment & Age of History & Inquiries & Label \\
       & Accounts &             & History & (days)         &           & \\\hline\hline
    1 & 8 & 15\% & 100\% & 1000 & 5 & GOOD \\\hline
    2 & 15 & 19\% & 90\% & 2500 & 8 & BAD \\\hline
    3 & 10 & 35\% & 100\% & 500 & 10 & BAD \\\hline
    4 & 11 & 40\% & 95\% & 2000 & 6 & BAD \\\hline
    5 & 12 & 10\% & 99\% & 3000 & 6 & GOOD \\\hline
    6 & 18 & 15\% & 100\% & 2000 & 5 & GOOD \\\hline
    7 & 3 & 21\% & 100\% & 1500 & 7 & BAD \\\hline
    8 & 14 & 4\% & 100\% & 3500 & 5 & GOOD \\\hline
    9 & 13 & 5\% & 100\% & 3000 & 3 & GOOD \\\hline
    10 & 6 & 25\% & 94\% & 2800 & 9 & BAD \\\hline
  \end{tabular}
\end{table}


Consider the following three accounts to be labeled:

\begin{table}[h]
  \centering
  \caption{Raw unlabeled accounts data for problem~\#1}\label{tab:P01:RawUnlabel}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Total    & Utilization & Payment & Age of History & Inquiries & Label \\
    Accounts &             & History & (days)         &           & \\\hline\hline
    20 & 50\% & 90\% & 4500 & 12 & P1 \\\hline
    8 & 10\% & 100\% & 550 & 4 & P2 \\\hline
    9 & 13\% & 99\% & 3000 & 6 & P3 \\\hline
  \end{tabular}
\end{table}
\end{problem}

\begin{subproblem}
  Before using nearest neighbor methods to make predictions, how would you recommend processing or transforming the data? Why? Make any changes you think appropriate to the data before continuing on to the next two parts.
\end{subproblem}

K-NN relies on a distance metric to quantify similarity between two examples.  If features are not changed to a consistent scales, necessarily some features will have higher weight than other.  In some cases, that may be advantageous but doing so blindly is dangerous and may yield dubious classifications.

The two most common approaches for transforming features are \textit{standardization} and \textit{normalization}.  Normalization is sensitive to outliers but creates relatively well-bounded feature ranges.  Since this data appears free of outliers, it is used for this problem. The \featureOp\ data for Tables~\ref{tab:P01:RawTrain} and~\ref{tab:P01:RawUnlabel} are in Tables~\ref{tab:P01:NormalTrain} and~\ref{tab:P01:NormalUnlabel} respectively.

\begin{table}[ht]
  \centering
  \caption{Problem~\#\arabic{problemCount} \featureOp\ training data}\label{tab:P01:NormalTrain}
  \begin{tabular}{|c||c|c|c|c|c|c|}
    \hline
    ID & Total    & Utilization & Payment & Age of History & Inquiries & Label \\
       & Accounts &             & History & (days)         &           & \\\hline\hline
    \input{src/tables/p01_train_normalized}
  \end{tabular}
\end{table}

\begin{table}[ht]
  \centering
  \caption{Problem~\#\arabic{problemCount} \featureOp\ unlabeled data}\label{tab:P01:NormalUnlabel}
  \begin{tabular}{|c|c|c|c|c||c|}
    \hline
    Total    & Utilization & Payment & Age of History & Inquiries & ID \\
    Accounts &             & History & (days)         &           & \\\hline\hline
    \input{src/tables/p01_unlabeled_normalized}
  \end{tabular}
\end{table}

\FloatBarrier
\begin{subproblem}
  What are the predicted labels P1, P2, and P3 using 1-NN with $L_1$ distance? Assume that percentages are represented as their corresponding decimal numbers, so 95\% = 0.95. Show your work.
\end{subproblem}

The predicted labels for the unlabeled data in Table~\ref{tab:P01:NormalUnlabel} is shown in Table~\ref{tab:P01:b:PredictedLabels}.  The calculations of the $L_1$ loss for P1, P2, and P3 are in Tables~\ref{tab:P01:b:P1},~\ref{tab:P01:b:P2}, and~\ref{tab:P01:b:P3} respectively.  Observe that for each unlabeled example, the minimum $L_1$ loss is in bold.

\begin{table}[h]
  \centering
  \caption{Predicted labels for Problem~\arabic{problemCount}(\alph{subProbCount})}\label{tab:P01:b:PredictedLabels}
  \begin{tabular}{|c|c|}
    \hline
    Example & Predicted Label \\\hline\hline
    \input{src/tables/p01_part_b_pred}
  \end{tabular}
\end{table}

\@for\id:={1,2,3}\do{
  \begin{table}[ht]
    \centering
    \caption{$L_1$ loss by feature for the \featureOp\ P\id\ data}\label{tab:P01:b:P\id}
    \begin{tabular}{|c||c|c|c|c|c||c|}
      \hline
      ID & Total    & Utilization & Payment & Age of History & Inquiries & Total \\
         & Accounts &             & History & (days)         &           & Loss \\\hline\hline
      \input{src/tables/p01_part_b_p\id_err}
    \end{tabular}
  \end{table}
}

\FloatBarrier
\begin{subproblem}
  Keep the information of customers 7, 8, 9, and 10 as validation data, and find the best K value for the K-NN algorithm. If the best value of $K$ is not equal to 1, find the new predictions for P1, P2, and P3. Show your work.
\end{subproblem}


