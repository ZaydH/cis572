\begin{problem}
  Suppose we have some binary input data, ${x_i \in \{0, 1\}}$. The training data is shown in Table~\ref{tab:P03:TrainingData}.


\noindent
Note that we have two outputs per training example.

Let us first embed each $x_i$ into 2D using the following basis function:

\[\phi(0) = (1, 0)^{T}, \phi(1) = (0, 1)^{T} \]

Consider a linear regression model over $\phi$:

\[\hat{\mathbf{y}} = \mathbf{W}^{T} \phi(x) \]

\noindent
where $\mathbf{W}$ is a $2\times2$ matrix. Compute $\mathbf{W}$ that has the maximum likelihood on the training data (called the maximum likelihood estimation). Show your calculation details.

\begin{table}[h]
  \centering
  \caption{Training data for question~\#3}\label{tab:P03:TrainingData}
  \begin{tabular}{c|c}
    x & y \\\hline
    0 & $(-1, -1)^{T}$\\
    0 & $(-1, -2)^{T}$\\
    0 & $(-2, -1)^{T}$\\
    1 & $(1, 1)^{T}$\\
    1 & $(1, 2)^{T}$\\
    1 & $(2, 1)^{T}$\\
  \end{tabular}
\end{table}

(Hint: \textnormal{Use ordinary least squares to find the analytic solution. The usual output vector over training data y we have in class is now a matrix.})
\end{problem}

Since $\phi$ maps $x_i$ onto the standard basis for $\mathbb{R}^2$, i.e.,~elementary vectors ${(1,0)}^{T}$ and ${(0,1)}^{T}$, the problem is significantly simplified.  This is due to the fact that the prediction for case $x=0$ only relies on the first row of $\mathbf{W}$ while the $x=1$ only relies on the second row of $\mathbf{W}$.

\noindent
\textbf{Case~1}: $x=0$

\[ J = \sum_{i=1}^{m} \sum_{j=1}^{2} \left(w_{j} - y_{i,j}\right)^2 \]

\noindent
Take the gradient of $J$ and set it to the $\vec{\mathbf{0}}$.

\[
\begin{bmatrix}
  \sum_{i=1}^m 2\lvert w_1 - y_{i,1}\rvert \\
  \sum_{i=1}^m 2\lvert w_2 - y_{i,2}\rvert
\end{bmatrix} =
\begin{bmatrix}
0 \\
0
\end{bmatrix}
\]

\noindent
Since all $y_{i,j}$ have the same sign, the absolute values can be dropped.  The problem can be solved to show:

\begin{aligncustom}
  \begin{bmatrix}
    3w_1\\
    3w_2
  \end{bmatrix} &=
  \begin{bmatrix}
    -4 \\
    -4
  \end{bmatrix} \\
  \begin{pmatrix}
    w_1 & w_2
  \end{pmatrix}^{T} &=
  \begin{pmatrix}
    -\frac{4}{3} & -\frac{4}{3}
  \end{pmatrix}^{T}
\end{aligncustom}

It is trivial to verify this critical point is a minimum and not a negative by trying values on either side.  Likelihood is maximized where MSE is minimized completing this case.

\noindent
\textbf{Case~2}: $x=1$

This case is identical to Case~1 except that the signs are positive.  Therefore, the weight vector is:

\[
  \begin{pmatrix}
    w_1 & w_2
  \end{pmatrix}^{T} =
  \begin{pmatrix}
    \frac{4}{3} & \frac{4}{3}
  \end{pmatrix}^{T}
\]

\noindent
\textbf{Conclusion}: The final value of $\mathbf{W}$ is formed form the concatenation of the vectors in Case~1 and Case~2 and is shown below.

\[
\boxed{
  \mathbf{W} =
  \begin{bmatrix}
    -\frac{4}{3} & -\frac{4}{3} \\
     \frac{4}{3} &  \frac{4}{3}
  \end{bmatrix}
}
\]
