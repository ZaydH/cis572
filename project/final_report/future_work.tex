\section{Future Work}\label{sec:FutureWork}

Although Section~\ref{sec:Experiments}'s experimental results are promising, perhaps the most beneficial aspect of this project is that it provided an opportunity to identify defects with our current algorithm which will allow for new future research directions.

First, observe that in Figure~\ref{fig:Toolname}, subvector~${\zS\in\zBase}$ is shared between the positive and negative decoders.  However, line~1 of Algorithm~\ref{alg:Complete} trains the encoder and negative decoder as a standard autoencoder.  The algorithm as designed never incentivizes the system to place the mutual information shared between the positive and negative classes into~$\zS$.  This necessitates one of two options either $\zS$ needs to be entirely removed or training algorithm needs to be explicitly modified to take advantage of it.

One question asked after the in-class presentation focused on whether additional datasets may be considered.  In response, we implemented support for \CIFARten.  However, initial results are poor and do not merit reporting; we further expect CIFAR-10 images will require convolutional layers for optimal performance.  The current timeline for initial \CIFARten\ results is the end of June.  Both the accuracy and AUC results are expected to be substantially worse for \CIFARten.
