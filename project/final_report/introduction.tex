\section{Introduction}

Consider classic supervised binary classification.  A learner is fit to a training set consisting of labeled examples from the positive and negative classes.  In contrast, \textit{Positive-unlabeled} (PU) \textit{learning} is a form of \textit{partially-supervised} binary classification where labeled data exists for only one class, i.e.,~positive.  The training set consists of a (positive) labeled set,~$\Pos$, and an unlabeled set,~$\Unlabel$, which is composed of both positive and negatives examples all of whose labels are unknown.  By convention, ${\Pos\cap\Unlabel=\emptyset}$.  PU~learning is applicable to both the \textit{transductive} setting, where the goal is to label~$\Unlabel$ as accurately as possible, as well as the \textit{inductive} setting, where the objective is to maximize the generalization performance on an unseen test set.

There are many applications where labeled data for one class may be unavailable or prohibitively expensive/difficult to collect. Domains where PU~learning has been applied include: land-cover classification~\cite{Li:2011}, protein similarity prediction~\cite{Elkan:2008}, disease gene identification~\cite{Yang:2012}, deceptive/incentivized review identification~\cite{Ren:2014}, targeted marketing~\cite{Yi:2017}, and prescription drug interaction analysis~\cite{Liu:2017}. The PU~learning framework can also be used for outlier detection given a set of inlier examples.~\cite{Scott:2009}

State-of-the-art PU learning algorithms generally rely on a cost-sensitive learning framework where each unlabeled example is simultaneously treated as both positive \textit{and} negative valued with each instance's class weights proportional to its specific label confidence.~\cite{Elkan:2008}  These previous works generally made one or more assumptions about the composition of~$\Pos$ and~$\Unlabel$. Most PU~learning paradigms use more traditional machine learning algorithms such as support vector machines~\cite{Elkan:2008} and logistic regression~\cite{Lee:2003}.  We are not aware of any previous PU~research that specifically focused on leveraging the significant, recent advances in deep learning.

The primary contribution of this work is \textit{\toolname}, a new autoencoder-based positive-unlabeled learner.  The remainder of this document is structured as follows.  Section~\ref{sec:Siamese} describes the Siamese neural network, which inspired this work.  Section~\ref{sec:Toolname} describes \toolname's neural architecture and reviews the novel learner's training algorithm including our custom PU~loss functions.  Section~\ref{sec:Experiments} describes our experimental setup and compares the performance of our algorithm against a~PU and supervised baseline.  Section~\ref{sec:FutureWork} outlines deficiencies identified with our algorithm and proposes future work that may address them.

