\documentclass[11pt,dvipsnames,usenames,aspectratio=169]{beamer}  % Add handout to options to disable overlays

% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{%
  \usetheme{CambridgeUS}    % or try default, Darmstadt, Warsaw, ...
  \usecolortheme{whale}     % or try albatross, beaver, crane, ...
  \usefonttheme{serif}          % or try default, structurebold, ...
  % \usefonttheme[onlymath]{serif}
  % \setbeamertemplate{navigation symbols}{}
  % \setbeamercovered{transparent}

  \setbeamercolor{title}{fg=white}
  \setbeamerfont{title}{series=\bfseries}
  \setbeamercolor{frametitle}{fg=black}
  \setbeamerfont{frametitle}{series=\bfseries}

  \setbeamercolor{section in head/foot}{fg=white}
  \setbeamerfont{section in head/foot}{series=\bfseries}
  \setbeamercolor{subsection in head/foot}{fg=white}
  \setbeamerfont{subsection in head/foot}{series=\bfseries}
  \setbeamercolor{author in head/foot}{fg=white}
  \setbeamerfont{author in head/foot}{series=\bfseries}
  \setbeamercolor{title in head/foot}{fg=white}
  \setbeamerfont{title in head/foot}{series=\bfseries}

  \setbeamercolor{block title}{use=structure,fg=white,bg=title in head/foot.bg}
  \setbeamerfont{block title}{series=\bfseries}
  \setbeamercolor{block body}{use=structure,fg=black,bg=black!1!white}
}

% Support graying out frame elements
\newcommand{\FrameOpague}{\setbeamercovered{again covered={\opaqueness<1->{40}}}}
% Transition slide
\newcommand{\transitionFrame}[1]{%
{%
  \begin{frame}[plain,noframenumbering]{}{} % the plain option removes the sidebar and header from the title page
    \setbeamertemplate{final page}[text]{\Large \textbf{#1}}
    \usebeamertemplate{final page}
  \end{frame}}
}

% \usepackage{hyperref}     % Loaded automatically by beamer
\usepackage{pgfplots}       % Used to generate embedded plots
\pgfplotsset{compat=1.13}

% Imported via UltiSnips
\usepackage{mathtools} % for "\DeclarePairedDelimiter" macro
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
% Imported via UltiSnips
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amsfonts}  % Used for \mathbb and \mathcal
\usepackage{amssymb}
% Imported via UltiSnips
\usepackage{tikz}
\usetikzlibrary{arrows.meta,decorations.markings,shadows,positioning,calc,backgrounds,shapes,overlay-beamer-styles}
% Packages used for the Convolutional NN drawing
\usepackage{graphicx}
\graphicspath{{./img/}}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{pgf-umlsd}
\usepackage{ifthen}
% Imported via UltiSnips
\usepackage[noend]{algpseudocode}
\usepackage[Algorithm,ruled]{algorithm}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\newcommand{\pos}{\mathcal{P}}
\newcommand{\unlabel}{\mathcal{U}}

\usepackage{color}
\newcommand{\blue}[1]{{\color{Blue} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\green}[1]{{\color{ForestGreen} #1}}

\input{macros}

% Here's where the presentation starts, with the info for the title slide
\title[Deep Positive-Unlabeled Learning]{Positive-Unlabeled Learning using a Deep Hybrid Generative/Discriminative Model}
\author[Zayd Hammoudeh]{%
  \href{mailto:zayd@cs.uoregon.edu}{\textbf{Zayd Hammoudeh}}\inst{1\textsuperscript{*}}  % \textsuperscript{(\Letter)}
  % \and
  % \href{mailto:lowd@cs.uoregon.edu}{Daniel Lowd}\inst{1}
}

\institute[Univ.\ Oregon]{%
  \textsuperscript{1}\textbf{University of Oregon}\\
  Eugene, OR, USA\\
  \texttt{\href{mailto:zayd@cs.uoregon.edu}{zayd@cs.uoregon.edu}}
  % \texttt{{zayd, lowd}@ucsc.edu}
}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

  % \vspace{20pt}
  \begin{center}
    \textsuperscript{*} Under the supervision of \textbf{Daniel Lowd}
  \end{center}
\end{frame}

\begin{frame}{What is PU Learning?}
  \begin{itemize}[<+->]
    \setlength{\itemsep}{14pt}
    \item PU = Positive-Unlabeled
    \item Form of \blue{\textbf{binary classification}}
    % \item Example of \textit{partially-supervised learning}
    \item \green{\textbf{Non-traditional}} training dataset $\mathcal{S} \coloneqq \pos \cup \unlabel$ such that $\pos \cap \unlabel = \emptyset$
      \begin{itemize}[<+->]
        \item $\pos$: Labeled examples all from positive class
        \item $\unlabel$: Unlabeled training set with \blue{\textbf{unknown distribution}} of positive \& negative examples
      \end{itemize}
    \item \textit{Example Applications}: %Protein-similarity prediction, land-cover classification, targeted marketing, deceptive/incentivized review identification
      \begin{itemize}[<+->]
        \item Both \blue{inductive} \& \red{transductive} domains
        \item We will focus on the \red{transductive} learning
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{What is an Autoencoder?}
  \onslide<+->{Neural \textbf{Generative Model}: Reconstructs input (image) from compressed representation}

  % \onslide<+->{\vspace{8pt}Used for \textit{Representation Learning} --- typically dimensionality reduction}
  \vspace{14pt}
  \input{tikz/cae.tex}
  % \\
  % \onslide<+->{\centering\Large Objective Function $J = \min \norm{\bfx - \mathbf{\hat{x}}}$}
\end{frame}

\begin{frame}{Basic Idea}
  \begin{itemize}[<+->]
    \setlength{\itemsep}{20pt}
    \item \textbf{Recall}: PU Learning is Binary Classification
      \begin{itemize}[<+->]
        \item \textit{Example}: \blue{\textbf{Positive}}=Cats \& \red{\textbf{Negative}}=Dogs
      \end{itemize}
    \item \textit{Project Intuition}: Simultaneously train two autoencoders (AE):
      \begin{itemize}[<+->]
        \setlength{\itemsep}{6pt}
        \item \textit{Positive AE}: Only reconstruct \blue{positive} examples \onslide<+->{(images of \blue{cats})}
        \item \textit{Negative AE}: Only reconstruct \red{negative} examples \onslide<+->{(images of \red{dogs})}
      \end{itemize}

    \item \textbf{Classification}: Assign label matching whichever autoencoder reconstructs input better
  \end{itemize}
\end{frame}

\begin{frame}{Our Architecture}
  \begin{center}
    \scalebox{0.65}{\input{tikz/deep_pu.tex}}
  \end{center}

  \blue{\textbf{Components}}:
  \begin{itemize}[<+->]
    \setlength{\itemsep}{4pt}
    \item Shared input encoder
    \item Latent vector $\mathbf{z}$ %partitioned into three parts
    \item $g_{p}$: Decoder for \blue{positive}-examples
    \item $g_{n}$: Decoder for \red{negative}-examples
  \end{itemize}
\end{frame}

\begin{frame}{Custom Loss Functions}
  ``\blue{\textbf{Attractive}}'' Loss for~$\pos$
  \begin{equation}\label{eq:Loss:AttP}
    \lPosAtt = \max\Big\{ \puDistDiff + \alpha, 0 \Big\}
  \end{equation}

  \vfill
  ``\green{\textbf{Pick-a-Side}}'' Loss for~$\unlabel$
  \begin{equation}\label{eq:Loss:AttU}
    \lUAtt = \max\Big\{ - \big\lvert\puDistDiff\big\rvert + \alpha, 0 \Big\}
  \end{equation}

  \vfill
  Unfortunately, there is no time now to discuss these, but we can in Q\&A if interested\ldots
\end{frame}

\begin{frame}{Training Algorithm (Sketch)}
  \input{alg/complete_alg}
\end{frame}

\begin{frame}{Experiments}
  \begin{itemize}
    \setlength{\itemsep}{16pt}
    \item \blue{\textbf{Dataset}}: MNIST
      \begin{itemize}
        \item \textit{Positive Class}: $4$
        \item \textit{Negative Class}: $9$
      \end{itemize}
    \item \blue{\textbf{Baseline}}: Elkan \& Noto with Logistic Regression
    \item \blue{\textbf{Dataset Sizes}}:
      \begin{itemize}
        \item $\abs{\pos}$: ${\sim}$3,000
        \item $\abs{\unlabel}$: ${\sim}$6,000 evenly split between positive \& negative classes
      \end{itemize}
    \item \blue{\textbf{Labeling Frequency}}: 50\%
  \end{itemize}

  \vspace{16pt}
  Let's look how an example matches our earlier intuition\ldots
\end{frame}

\begin{frame}{Decoder Outputs --- Epoch \red{0}}
  \begin{columns}
    \begin{column}{0.59\textwidth}
      \begin{center}
        \includegraphics[scale=0.44]{deep-pu_epoch=000.jpg}
      \end{center}
    \end{column}
    \begin{column}{0.41\textwidth}
      \begin{itemize}[<+->]
        \setlength{\itemsep}{20pt}
        \item Positive decoder reconstructs everything as~4
        \item Negative decoder reconstructs both~4 and~9
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Decoder Outputs --- Epoch \red{1}}
  \begin{columns}
    \begin{column}{0.59\textwidth}
      \begin{center}
        \includegraphics[scale=0.44]{deep-pu_epoch=001.jpg}
      \end{center}
    \end{column}
    \begin{column}{0.41\textwidth}
      \begin{itemize}[<+->]
        \setlength{\itemsep}{20pt}
        \item Not much change after one epoch
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Decoder Outputs --- Epoch \red{10}}
  \begin{columns}
    \begin{column}{0.59\textwidth}
      \begin{center}
        \includegraphics[scale=0.44]{deep-pu_epoch=010.jpg}
      \end{center}
    \end{column}
    \begin{column}{0.41\textwidth}
      \begin{itemize}[<+->]
        \setlength{\itemsep}{20pt}
        \item Positive decoder still unchanged
        \item Negative decoder starting to close \blue{$4$}s to form \red{$9$}s
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Decoder Outputs --- Epoch \red{20}}
  \begin{columns}
    \begin{column}{0.59\textwidth}
      \begin{center}
        \includegraphics[scale=0.44]{deep-pu_epoch=020.jpg}
      \end{center}
    \end{column}
    \begin{column}{0.41\textwidth}
      \begin{itemize}[<+->]
        \setlength{\itemsep}{20pt}
        \item Positive decoder still unchanged
        \item Negative decoder turning \blue{$4$}s into \red{$9$}s even more
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Decoder Outputs --- Epoch \red{50}}
  \begin{columns}
    \begin{column}{0.59\textwidth}
      \begin{center}
        \includegraphics[scale=0.44]{deep-pu_epoch=050.jpg}
      \end{center}
    \end{column}
    \begin{column}{0.41\textwidth}
      \begin{itemize}[<+->]
        \setlength{\itemsep}{20pt}
        \item Positive decoder still unchanged
        \item Negative decoder almost entirely closes input \blue{$4$}s
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Decoder Outputs --- Epoch \red{100}}
  \begin{columns}
    \begin{column}{0.59\textwidth}
      \begin{center}
        \includegraphics[scale=0.44]{deep-pu_epoch=050.jpg}
      \end{center}
    \end{column}
    \begin{column}{0.41\textwidth}
      \begin{itemize}[<+->]
        \setlength{\itemsep}{20pt}
        \item Positive \& negative decoders always reconstruct \blue{$4$}s \& \red{$9$}s respectively
        \item Suitable to use the generative model as a discriminative classifier
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Quantitative Results}
  \begin{itemize}
    \setlength{\itemsep}{15pt}
    \onslide<+->{
      \item \blue{\textbf{Our Architecture}}:
        \begin{itemize}
          \item \textit{Accuracy}: 98.0\%
          \item \textit{AUC ROC}: 0.996
          \item \textit{F1 Score}: 0.980
        \end{itemize}
    }
    \onslide<+->{
      \item \green{\textbf{Elkan \& Noto}}: Using Logistic Regression
        \begin{itemize}
          \item Discussion of this algorithm is beyond the scope of this talk.
          \item \textit{Accuracy}: 89.9\%
          \item \textit{AUC ROC}: 0.969
          \item \textit{F1~Score}: 0.932
        \end{itemize}
    }
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
  {\tiny
    \frametitle{References}
    \bibliographystyle{ieeetr}
    \bibliography{bib/ref.bib}
  }
\end{frame}

\appendix
\begin{frame}{Previous Work}
  \onslide<+->{\textbf{Two Primary Paradigms}}
  \begin{itemize}[<+->]
    \setlength{\itemsep}{16pt}
    \item ``\blue{\textbf{One Stage Approach}}''
      \begin{itemize}[<+->]
        \setlength{\itemsep}{4pt}
      \item \textit{Heuristically} extract ``reliable negative'' examples~$\mathcal{N}'\in\unlabel$
        \item Train binary classifier $\pos$ vs.\ $\mathcal{N}'$
        \item \red{Disadvantage}: Heuristic-based with high variance based on $\mathcal{N}'$ extraction
      \end{itemize}

    \item ``\blue{\textbf{Two Stage Approach}}''
      \begin{itemize}[<+->]
        \setlength{\itemsep}{4pt}
        \item Cost-sensitive optimization framework
        \item Assume all examples in~$\mathcal{U}$ are negative with a label weight proportional to confidence sample is negative
        \item Train binary classifier $\pos$ vs.\ $\unlabel$
      \end{itemize}
  \end{itemize}

  \vspace{10pt}
  \onslide<+->{\green{\textbf{Baseline for Comparison}}: Seminal two stage approach from Elkan \& Noto~\cite{Elkan:2008}}
\end{frame}

\end{document}
